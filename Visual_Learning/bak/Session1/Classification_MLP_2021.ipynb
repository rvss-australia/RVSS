{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvPV3WzCC6WL"
      },
      "source": [
        "# Image classification with multi-layer perceptron.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7hGKxuC6WN"
      },
      "source": [
        "## Import all the packages required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx3dzFZXC6WN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import time for timekeeping\n",
        "import time\n",
        "# io allows reading and writing image from disk\n",
        "# from skimage import io\n",
        "\n",
        "\n",
        "# Pytorch (Our Deep Learning Framework)\n",
        "import torch\n",
        "\n",
        "# Torch Data Loader (this will be helful to load image)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# datasets have mnist if using custom images import io from skimage\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "# stores different optimizors like SGD\n",
        "import torch.optim as optim\n",
        "\n",
        "# Some torch functions that are used multiple times\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set to False if not using GPUs\n",
        "FLAG_GPU = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jddXhqqC6WO"
      },
      "source": [
        "## Lets Define a Multi Layer Perceptronin \"__ init __ \" function\n",
        "* Any network has an * __ init __ * function that initializes all the layers on a NN that require learnable parameters.\n",
        "* A MPL is stack of fully connected layers. In this example we use three fully connected layers named :''fc0'', ''fc1'' and ''fc2''.\n",
        "* Note that each fully connected layer has a number of input neurons that connect to a number of output neurons. \n",
        "* These input and output dimenssions are specified in fc layers initialization.\n",
        "* If a fully connected layers connect to another, its output size = input size of fully connected layer that followes.\n",
        "* Number of paramenters in any fully connected layer is #Input x #Output (and 1 bias per output).\n",
        "\n",
        "## How do we write a forward function?\n",
        "* torch.flatten(x, start_dim = dim) converts an image like entity to a vector.\n",
        "* Remeber that you need activations after every fc layer. In this case ReLu. \n",
        "* Notice the log_sofmax layer at the end. This is a softmax activation function followed by log function as name suggests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axo8Z6r0C6WP"
      },
      "outputs": [],
      "source": [
        "class MLPNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPNet, self).__init__()\n",
        "        \n",
        "        # First fully connected layers input image is 28x28 = 784 dim.\n",
        "        self.fc0 = nn.Linear(784, 256) # nparam = 784*256 = 38400\n",
        "        # Two more fully connected layers\n",
        "        self.fc1 = nn.Linear(256, 84)\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flattens the image like structure into vectors\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        # fully connected layers with activations\n",
        "        x = self.fc0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # Outputs are log(p) so softmax followed by log.\n",
        "        #return(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4dxbA4JC6WP"
      },
      "source": [
        "## Initializing a instance of the defined network here.\n",
        "* Note that puting a network to GPU is as simple as writing .cuda() at the end of the instance.\n",
        "* Same is true for a variable. In this  notebook the code inside command \"if FLAG_GPU\" shows all the modifications you need to run your code on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z_buonSC6WP"
      },
      "outputs": [],
      "source": [
        "net = MLPNet()\n",
        "if FLAG_GPU:\n",
        "    net.cuda()\n",
        "    print(net)\n",
        "else:\n",
        "    print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUli0X3PC6WQ"
      },
      "source": [
        "## Dataloaders and Transforms.\n",
        "* dataset.MNIST in pytorch has functionality to download and process MNIST data.\n",
        "* dataloader function usually allows for loading parts of training and test data in minibatches.\n",
        "* It can use somple simple transformations implemented in class transforms that assists training. For example normalizing, resizing or cropping images.\n",
        "* Functionality to dataset, transforms and dataloader classes are usually added to suit new data and training proceedure related to the problem at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sSIHCwOC6WQ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "# Training dataset and training loader.\n",
        "trainset = datasets.MNIST(root='../data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "# Test dataset and loader.\n",
        "testset = datasets.MNIST(root='../data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LapfjghXC6WQ"
      },
      "source": [
        "## Here we see sample usage of loading some MNIST training data.\n",
        "* How does out training minibatch looks?\n",
        "* At times simple visualization and print statements allowes for understanding/debugging effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzvzjznjC6WQ"
      },
      "outputs": [],
      "source": [
        "def imshow(img, l):\n",
        "    img = img/2  + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    #plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "    print('Labels were:')\n",
        "    print(l.reshape(-1,8).numpy())\n",
        "\n",
        "# Load sample data\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print('shape of images', images.shape)\n",
        "\n",
        "# display batch\n",
        "imshow(utils.make_grid(images),labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGg8FQL5C6WQ"
      },
      "source": [
        "## Loss function for learning.\n",
        "* NLLLoss: The abbrivation NLL stands for Negetive log likelihood. It is however a bit of misnomer as the log is not included in the loss itself but was part of the network defination above. \n",
        "* NOTE: When you want to get the probability/likelihood of an image being of a perticular class you need to remove the log from the forward function and use simple softmax activation at test time. Alternatively simply use ''exp'' function from torch to invert log and leave the forward function as it is. \n",
        "\n",
        "## Optimizer\n",
        "* pytorch have various optimization rutines (beyond SGD) pre-implemented.\n",
        "* class optim will take care of backpropogation with these different optimizations for learning as long as the network defination with appropriate forward function is written correctly.\n",
        "* Here we just use SGD. with learning rate 0.001 and momentum 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-HaZbNEC6WR"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "if FLAG_GPU:\n",
        "    criterion = criterion.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5jRjGRFC6WR"
      },
      "source": [
        "## This cell of the notebook is now training a network.\n",
        "\n",
        "* First for loop goes throught the entire data 5 times (We run 5 epochs for our training).\n",
        "* The simple steps for training a NN with pytorch are:\n",
        "    * Load data in minibatches.\n",
        "    * Set gradients for all the network parameters to zero (dont forget this)\n",
        "    * Pass data to the NN using a net.forward() to compute layer by layer output.\n",
        "        * Intermediate outputs can be returned as extra variables in forward function.\n",
        "    * Compute the loss from the output (remember it is defined above).\n",
        "    * Use loss.backword() to compute all the gradients by appropriately applying chain rule! \n",
        "        * It actually know how to differentiate things!!!\n",
        "    * Use optimizer.step() updates weights.\n",
        "    \n",
        "## At the end of every epoch usually we check if NN generalizes.\n",
        "* Generalization is critical in learning.\n",
        "* We evaluate the performance of our NN on new data, for which the NN loss was not minimized.\n",
        "* torch.no_grad() command forces the following code to not keep track of the gradients as for testing we dont need them.\n",
        "* As no gradients are maintained, the code runs faster!\n",
        "* It a very good practice to make use of no_grad function to ensure that we dont accidently minimize loss on the data we are testing the performance on.\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGchJPitC6WR"
      },
      "outputs": [],
      "source": [
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    # Simply for time keeping\n",
        "    start_time = time.time()\n",
        "    # Loop over all training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        " \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward \n",
        "        if FLAG_GPU:\n",
        "            outputs = net(inputs.cuda())\n",
        "            loss = criterion(outputs, labels.cuda())\n",
        "        else:\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Compute Gradients\n",
        "        loss.backward()\n",
        "        # BackProp\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "        # endif\n",
        "    # end for over minibatches epoch finishes\n",
        "    end_time = time.time()\n",
        "\n",
        "    # test the network every epoch on test example\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Test after the epoch finishes (no gradient computation needed)\n",
        "    with torch.no_grad():\n",
        "        VIS = True\n",
        "        for data in testloader:\n",
        "            # load images and labels\n",
        "            images, labels = data\n",
        "\n",
        "            if FLAG_GPU:\n",
        "                outputs = net(images.cuda())\n",
        "                # note here we take the max of all probability\n",
        "                _, predicted = torch.max(outputs.cpu(), 1)\n",
        "            else:\n",
        "                outputs = net(images)\n",
        "                # note here we take the max of all probability\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "      #end for\n",
        "    #end with\n",
        "    print('Epoch', epoch+1, 'took', end_time-start_time, 'seconds')\n",
        "    print('Accuracy of the network after', epoch+1, 'epochs is' , 100*correct/total)\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jJDBPIPC6WS"
      },
      "outputs": [],
      "source": [
        "iterloader = iter(testloader)\n",
        "data = next(iterloader)\n",
        "\n",
        "# load images and labels\n",
        "images, labels = data\n",
        "\n",
        "if FLAG_GPU:\n",
        "    outputs = net(images.cuda())\n",
        "    # note here we take the max of all probability\n",
        "    _, predicted = torch.max(outputs.cpu(), 1)\n",
        "else:\n",
        "    outputs = net(images)\n",
        "    # note here we take the max of all probability\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "imshow(utils.make_grid(images),predicted)\n",
        "print('Correct labels:')\n",
        "print(labels.reshape(-1,8).numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imSnkceRD_A-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}