{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIiaZznhtXJx"
      },
      "source": [
        "# <center> Introduction to Model-Free Reinforcement Learning</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsAQl2ectXJy"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "#### Make sure that all the required dependencies are installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGckOaHOtXJy"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tobias-Fischer/RVSS2022.git"
      ],
      "metadata": {
        "id": "MJ7g9NpotxpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GOk-7urtXJz"
      },
      "source": [
        "#### Import all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqEZvZX_tXJz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('RVSS2022/Reinforcement_Learning/Support'))\n",
        "\n",
        "from gym_simple_gridworlds.envs.grid_env import GridEnv\n",
        "from gym_simple_gridworlds.envs.grid_2dplot import *\n",
        "from gym_simple_gridworlds.helper import *\n",
        "from gym_simple_gridworlds.dp_algorithms import *\n",
        "\n",
        "from collections import namedtuple, defaultdict\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "from IPython.core.display import display, HTML, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1gYQB1ZtXJ0"
      },
      "source": [
        "# 2. Temporal Difference Learning - Prediction \n",
        "\n",
        "Recall the grid in which our robot lives\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/GridWorldExample.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
        "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
        "    - Up : (-1,0)\n",
        "    - Down: (1,0)\n",
        "    - Left: (0,-1)\n",
        "    - Right: (0, 1)\n",
        "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
        "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
        "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
        "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)\n",
        "\n",
        "This environment is represented with the class ``GridEnv``. **To a look at the attributes of this class, place the cursor somewhere on the class' name and hit SHIFT+TAB. If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**\n",
        "\n",
        "### Known Model\n",
        "\n",
        "Recall also the **optimal policy** we found yesterday using policy-interation\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/example_policy.png)"
      ],
      "metadata": {
        "id": "bgO6HuvQuOel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "since the dynamics of our grid world environment are known, we obtained the state-value function $v_\\pi(s)$ associated to this policy using ``policy_evalution(.)`` \n",
        "\n",
        "We have defined the class ``GridEnv`` to represent our Grid World MDP. **Take a look at the attributes of this class by placing the cursor somewhere on the class' name and hit SHIFT+TAB. If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**"
      ],
      "metadata": {
        "id": "0WaHfM6nuYF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd_8Gm6HtXJ0"
      },
      "outputs": [],
      "source": [
        "# Create a Grid World instance\n",
        "grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
        "\n",
        "# Get policy shown in image\n",
        "policy_pi = encode_policy(grid_world)\n",
        "\n",
        "# Compute value-function using dynamic programming\n",
        "v_pi = policy_evaluation(grid_world, policy_pi)\n",
        "\n",
        "plot_value_function(grid_world, v_pi)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Sx8Q7ntXJ0"
      },
      "source": [
        "### What if the Transition and Reward Function are Unknown?\n",
        "\n",
        "Let's first define the helper method ``generate_episode(.)``. It samples an episode i.e., a sequence of ($s, a, r, s'$) tuples, from a given policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ4wEilytXJ0"
      },
      "outputs": [],
      "source": [
        "Sample = namedtuple('Sample', ['state', 'action', 'reward', 'next_state'])\n",
        "\n",
        "def generate_episode(grid_env, policy):\n",
        "    \"\"\"\n",
        "    Generate an episode of experiences in environment under a given policy\n",
        "    :param grid_env (GridEnv): Environment\n",
        "    :param policy (dict of probabilites): Policy used to sample actions\n",
        "    \n",
        "    :return List(Sample) Complete episode\n",
        "    \"\"\"\n",
        "    episode = []\n",
        "\n",
        "    # Reset the environment to a random initial state\n",
        "    state = grid_env.reset()\n",
        "\n",
        "    # Set flag to indicate whether episode has ended\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Get actions available at current state\n",
        "        all_actions = list(policy_pi[state].keys())\n",
        "        # Get action probabilities\n",
        "        all_probabilities = np.array(list(policy[state].values()))\n",
        "        # Sample an action from policy\n",
        "        action = np.random.choice(all_actions, 1, p=all_probabilities)[0]\n",
        "        \n",
        "        next_state, reward, done, info = grid_env.step(action)\n",
        "        episode.append(Sample(state, action, reward, next_state))\n",
        "        state = next_state\n",
        "\n",
        "    return episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEk1U3jEtXJ1"
      },
      "source": [
        "Now, under the assumption that $\\mathcal{T}(s,a,s')$ and $\\mathcal{R}(s,a)$ are unknown, let's use the algorithm shown below to get an estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/TDPolicyEvaluation.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7btWVvgtXJ1"
      },
      "outputs": [],
      "source": [
        "def temporal_learning_policy_evaluation(grid_env, policy, true_v, alpha=0.1, n_episodes=1):\n",
        "    \"\"\"\n",
        "    Compute estimate of state-value function for a given policy\n",
        "    :param grid_env (GridEnv): Environment\n",
        "    :param policy (dict of probabilites): Policy to be evaluated\n",
        "    :param true_v (dict of floats): True state-value function. Used to compute prediciton error\n",
        "    :param alpha (float): step-size\n",
        "    :param n_episodes (int): Number of episodes to use for prediction\n",
        "    \n",
        "    :return List(float): Prediction error after each episode\n",
        "    :return dict(float): Predicted state-value function\n",
        "    \n",
        "    \"\"\"\n",
        "    all_states = grid_env.get_states()\n",
        "    \n",
        "    # Predicted state-value function\n",
        "    pred_v = {s:0 for s in all_states}\n",
        "    \n",
        "    # Variable used for plotting\n",
        "    list_errors = []\n",
        "    \n",
        "    for i in range(n_episodes):\n",
        "        # Generate episode\n",
        "        episode = generate_episode(grid_env, policy)\n",
        "        \n",
        "        # Variable used to keep track of prediction error for this episode\n",
        "        error = 0\n",
        "        \n",
        "        # Starting from the first sampled observation in episode\n",
        "        for obs in episode:\n",
        "            pred_v[obs.state] += alpha * (obs.reward + grid_env.gamma * pred_v[obs.next_state] - pred_v[obs.state]) \n",
        "            error += np.abs(true_v[obs.state] - pred_v[obs.state])       \n",
        "        \n",
        "        list_errors.append(error)\n",
        "    \n",
        "    return list_errors, pred_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmSDM9BJtXJ1"
      },
      "source": [
        "Let's now try the algorithm and compare its output to the true value-state function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t86VAp-6tXJ2"
      },
      "outputs": [],
      "source": [
        "errors, predicted_v = temporal_learning_policy_evaluation(grid_world, policy_pi, v_pi, alpha=0.1, n_episodes=200)\n",
        "\n",
        "fig = plt.figure(constrained_layout=True)\n",
        "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
        "f_ax1 = fig.add_subplot(spec[0, 0])\n",
        "f_ax2 = fig.add_subplot(spec[0, 1])\n",
        "f_ax3 = fig.add_subplot(spec[1, :])\n",
        "\n",
        "#Plot true value function\n",
        "plot_value_function(grid_world, v_pi, f_ax1)\n",
        "f_ax1.set_title(\"True state-value function\")\n",
        "\n",
        "plot_value_function(grid_world, predicted_v, f_ax2)\n",
        "f_ax2.set_title(\"Predicted state-value function\")\n",
        "\n",
        "f_ax3.plot(errors)\n",
        "f_ax3.set_title(\"Predicted Error (sum of abs. differences)\")\n",
        "f_ax3.set_xlabel(\"Num. episodes\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUj8BouwtXJ2"
      },
      "source": [
        "# 3. Temporal Difference Learning - Control\n",
        "\n",
        "In a model-free setting where our state-value and action-value estimates depend on the actions chosen by the agent, how can we guarantee that the all actions will continue to be selected?\n",
        "\n",
        "## 3.1 $\\epsilon$-Greedy Policies\n",
        "\n",
        "We can use an $\\epsilon$-greedy policy. This type of policy are formally defined as:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "    \\pi(a|s) = \n",
        "    \\begin{cases}\n",
        "        1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|},&  \\text{if } a^* = \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s,a)\\\\\n",
        "        \\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{otherwise}\n",
        "    \\end{cases}\n",
        "\\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "Let's see how the agent behaves when it follows an $\\epsilon$-greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvPGyxc1tXJ2"
      },
      "outputs": [],
      "source": [
        "def get_egreedy_action(grid_env, state, q_value, epsilon):\n",
        "    \"\"\"\n",
        "    Select action to execute at a given state under an epsilon-greedy policy\n",
        "    :param grid_env (GridEnv): Grid world environment\n",
        "    :param state (int): Location in grid for which next action is going to be choosen\n",
        "    :param q_value (dict): Action-value function \n",
        "    :param epsilon (float): Randomness threshold used to choose action\n",
        "    \"\"\"\n",
        "    \n",
        "    rand_n = np.random.random()\n",
        "    if rand_n <= epsilon:\n",
        "        return grid_env.action_space.sample()\n",
        "    else:\n",
        "        actions = list(q_value[state].keys())\n",
        "        return actions[np.argmax(list(q_value[state].values()))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbcPgIz5tXJ2"
      },
      "outputs": [],
      "source": [
        "# We set noise to zero. Randomness in agent behaviour is only due to e-greedy policy\n",
        "grid_world = GridEnv(noise=0.2, living_reward=-0.04, gamma=0.99)\n",
        "\n",
        "# Get policy shown in section 1\n",
        "policy_pi = encode_policy(grid_world)\n",
        "\n",
        "# Compute value-function using dynamic programming\n",
        "v_pi = policy_evaluation(grid_world, policy_pi)\n",
        "\n",
        "# Use value-function to compute q-values\n",
        "q_pi = grid_world.get_q_values(v_pi)\n",
        "\n",
        "# Start episode\n",
        "cur_state = grid_world.idx_cur_state\n",
        "s_x, s_y = get_state_to_plot(grid_world)\n",
        "\n",
        "# We can visualize our grid world using the render() function\n",
        "fig, ax = grid_world.render()\n",
        "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
        "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
        "\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "path_to_plot = []\n",
        "\n",
        "v_epsilon = 0.8\n",
        "\n",
        "while not done:\n",
        "    action = get_egreedy_action(grid_world, cur_state, q_pi, v_epsilon)\n",
        "    cur_state, cur_reward, done, _ = grid_world.step(int(action))\n",
        "    n_x, n_y = get_state_to_plot(grid_world)\n",
        "    cumulative_reward += cur_reward\n",
        "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
        "\n",
        "def init():\n",
        "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
        "    reward_text.set_text('')\n",
        "    return agent, reward_text\n",
        "\n",
        "def animate(i):\n",
        "    if i < len(path_to_plot):\n",
        "        r, n_x, n_y = path_to_plot[i]\n",
        "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
        "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
        "    return agent, reward_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
        "                              repeat=False)\n",
        "\n",
        "HTML(ani.to_html5_video())"
      ],
      "metadata": {
        "id": "CodK22_Byed1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNioafBZtXJ3"
      },
      "source": [
        "## 3.2 Q-Learning\n",
        "\n",
        "We have seen how to evaluate a policy without a model. Let's now find an *approximately* optimal policy using the off-policy control method Q-learning.\n",
        "\n",
        "To help during the learning, we have added a lambda function that iteratively decreases epsilon. Our agent will strongly explore the environment at first to then swicth into exploitation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Agb6ojIqtXJ3"
      },
      "outputs": [],
      "source": [
        "min_epsilon=0.001\n",
        "max_epsilon=1.0\n",
        "epsilon_decay = 80.0\n",
        "epsilon_by_episode = lambda ep_idx: min_epsilon + (max_epsilon - min_epsilon) * math.exp (-1 * ep_idx/epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiG6NmsntXJ3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "\n",
        "ax.plot([epsilon_by_episode(i) for i in range(500)])\n",
        "ax.set_xlabel(\"Num. episodes\")\n",
        "ax.set_ylabel(\"Epsilon\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNMOWgY3tXJ3"
      },
      "source": [
        "Here is our implementation of the q-learning algorithm shown below\n",
        "\n",
        "![](https://raw.githubusercontent.com/Tobias-Fischer/RVSS2022/main/Reinforcement_Learning/Support/images/TDPolicyEvaluation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete the missing steps**:\n",
        "- Choose an action using an $\\epsilon$-greedy policy (use the function ``get_egreedy_action(.)`` we tested in section 2)\n",
        "- Update our q-function using a greedy (max) policy (use ``q_function[cur_state][action]`` to index our q-function)"
      ],
      "metadata": {
        "id": "wsfdyMbnyr2S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aB_8qEotXJ3"
      },
      "source": [
        "**Keep in Mind**: Correspondance between the mathematical notation and implemented code\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "<thead>\n",
        "<tr>\n",
        "<th style=\"width:150px;font-size:20px;text-align:center;\">Notation</th>\n",
        "<th colspan=2, style=\"width:450px;font-size:20px;text-align:center;\">Code</th>\n",
        "</tr>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <th style=\"font-size:15px;text-align:center;\">Variable/Attribute</th>\n",
        "        <th style=\"font-size:15px;text-align:center;\">Type</th>\n",
        "    </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\epsilon$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">epsilon_by_episode</td>\n",
        "    <td>float</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\alpha$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">alpha</td>\n",
        "    <td>float</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\gamma$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">grid_world.gamma</td>\n",
        "    <td>float</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$\\hat{q}(s, a)$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">q_function[idx_s][idx_a]</td>\n",
        "    <td>dict of dict</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$s$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">cur_state</td>\n",
        "    <td>int</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$r$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">reward</td>\n",
        "    <td>int</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"text-align:left;\">$s'$</td>\n",
        "    <td style=\"text-align:left;font-size:15px;\">next_state</td>\n",
        "    <td>int</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSjFzyH0tXJ4"
      },
      "outputs": [],
      "source": [
        "def q_learning(grid_env, alpha=0.1, min_epsilon=0.01, max_epsilon=1.0, \n",
        "               epsilon_decay = 80.0, n_episodes=500):\n",
        "    \"\"\"\n",
        "    This function computes an approximately optimal policy using q-learning\n",
        "    \n",
        "    :param grid_env (GridEnv): MDP environment\n",
        "    :param alpha (float): step-size\n",
        "    :param epsilon (float): value used during e-greedy action selection\n",
        "    :return: (dict) State-values for all non-terminal states\n",
        "    \"\"\"\n",
        "        \n",
        "    # This lambda function iteratively decreases epsilon\n",
        "    epsilon_by_episode = lambda ep_idx: min_epsilon + (max_epsilon - min_epsilon) * math.exp (-1 * ep_idx/epsilon_decay)\n",
        "    \n",
        "    # Obtain list of all states in environment\n",
        "    states = grid_env.get_states()\n",
        "    actions = grid_env.get_actions()\n",
        "    q_function = defaultdict(lambda: defaultdict(float))\n",
        "    \n",
        "    # Initialize q_function arbitrarily\n",
        "    for s in states:\n",
        "        for a in actions:\n",
        "            q_function[s][a] = 0\n",
        "    \n",
        "    \n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        cur_state = grid_env.reset()\n",
        "        done = False\n",
        "        epsilon = epsilon_by_episode(i_episode)\n",
        "        \n",
        "        while not done:\n",
        "            # TODO 1: Complete off-policy action selection (e-greedy)\n",
        "            action = None\n",
        "\n",
        "            next_state, reward, done,_ = grid_env.step(action)\n",
        "            q_next_state = list(q_function[next_state].values())\n",
        "            \n",
        "            # TODO 2: Complete update of q-function\n",
        "            q_function[cur_state][action] += 0\n",
        "            \n",
        "            cur_state=next_state\n",
        "    \n",
        "    return decode_policy(grid_env, q_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnXbnJUgtXJ4"
      },
      "source": [
        "Let's now test our implementation and compare our free-model policy with the one we obtained using value iteration (another DP algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWYxpPxmtXJ4"
      },
      "outputs": [],
      "source": [
        "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
        "q_learning_policy = q_learning(grid_world)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcz6DmK3tXJ4"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n",
        "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
        "f_ax1 = fig.add_subplot(spec[0, 0])\n",
        "f_ax2 = fig.add_subplot(spec[0, 1])\n",
        "f_ax3 = fig.add_subplot(spec[1, 0])\n",
        "f_ax4 = fig.add_subplot(spec[1, 1])\n",
        "\n",
        "#Plot policy obtained using value-iteration value function\n",
        "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0)\n",
        "value_function, optimal_policy = value_iteration(grid_world)\n",
        "\n",
        "plot_policy(grid_world, optimal_policy, f_ax1)\n",
        "f_ax1.set_title(\"Policy - Value Iteration\")\n",
        "\n",
        "plot_policy(grid_world, q_learning_policy, f_ax2)\n",
        "f_ax2.set_title(\"Policy - Q-learning\")\n",
        "\n",
        "# Compute value function for q_learning policy\n",
        "q_policy_state_values = policy_evaluation(grid_world, encode_policy(grid_world, q_learning_policy))\n",
        "\n",
        "plot_value_function(grid_world, value_function, f_ax3)\n",
        "f_ax3.set_title(\"Value Function - Value Iteration\")\n",
        "\n",
        "plot_value_function(grid_world, q_policy_state_values, f_ax4)\n",
        "f_ax4.set_title(\"Value Function - Q-learning\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zjBpagmTzD-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "IntelligentRobotics",
      "language": "python",
      "name": "intelligentrobotics"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "ModelFreeRL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
