{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_0aEaFYzu5L"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://github.com/rvss-australia/RVSS/blob/main/Pics/RVSS-logo-col.med.jpg?raw=1\" width=\"400\"></td>\n",
        "    <td><div align=\"left\"><font size=\"30\">Image Features</font></div></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "(c) Peter Corke 2024\n",
        "\n",
        "Robotics, Vision & Control: Python, see Chapter 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJjzn-jIzu5O"
      },
      "source": [
        "## Configuring the Jupyter environment\n",
        "We need to import some packages to help us with linear algebra (`numpy`), graphics (`matplotlib`), and machine vision (`machinevisiontoolbox`).\n",
        "If you're running locally you need to have these packages installed.  If you're running on CoLab we have to first install machinevisiontoolbox which is not preinstalled, this will be a bit slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "puZNEjc1zu5O"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    print('Running on CoLab')\n",
        "    !pip install machinevision-toolbox-python\n",
        "    COLAB = True\n",
        "except:\n",
        "    COLAB = False\n",
        "\n",
        "%matplotlib ipympl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from machinevisiontoolbox import Image, Kernel\n",
        "from spatialmath.base import plot_point\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ9EiqPWzu5Q"
      },
      "source": [
        "***\n",
        "\n",
        "We will work with the greyscale Mona Lisa image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CUVLfepzu5R"
      },
      "outputs": [],
      "source": [
        "mona = Image.Read('monalisa.png', grey=True)\n",
        "mona.disp();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRxaSHzNZ0h"
      },
      "source": [
        "# Histogram\n",
        "\n",
        "We can show the distribution of grey values within the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0spQHKVNTEW"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "mona.hist().plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tozbf5_NvME"
      },
      "source": [
        "and also display some simple statistics of those grey values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LyfmkJUNpSi"
      },
      "outputs": [],
      "source": [
        "mona.stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-XYnEYBzu5R"
      },
      "source": [
        "# Spatial operators\n",
        "## Image smoothing by averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIA8ggA8zu5R"
      },
      "source": [
        "The image of the Mona Lisa looks rather grainy, the paint is cracked, but she is over 500 years old...\n",
        "\n",
        "We could smooth it out by local averaging, where every pixel in the output image is the average of all pixels in a $5 \\times 5$ window about the corresponding input pixel.  We first create a kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lBg-u5Izu5S"
      },
      "outputs": [],
      "source": [
        "kernel = np.ones((5,5),np.float32) / 25\n",
        "kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dme_Qtmszu5S"
      },
      "source": [
        "where the sum of all values is equal to one.  For a given input pixel, say (20,30), the $5 \\times 5$ window is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NddcRo3Jzu5S"
      },
      "outputs": [],
      "source": [
        "window = mona.image[30-2:30+3,20-2:20+3]\n",
        "window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egygqQJzzu5S"
      },
      "source": [
        "<p style=\"border:3px; border-style:solid; border-color:#FF0000; padding: 1em;\">Note that we swapped the 20 and 30.  (20,30) means the horizontal coordinate is 20 and the vertical coordinate is 30, but when we index into a 2D array the first index is row (vertical direction) and the second index is column (horizontal direction).</p>\n",
        "<p style=\"border:3px; border-style:solid; border-color:#FF0000; padding: 1em;\">Note that the range 30-2:30+3 includes 30-2, 30-1, 30-0, 30+1, 30+2.  In Python the end value of  a range is not included in the set.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poL9PSNFzu5T"
      },
      "outputs": [],
      "source": [
        "z = window * kernel  # using element-wise multiplication\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WZjGG4Bzu5T"
      },
      "outputs": [],
      "source": [
        "np.sum(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67dUoTVdzu5T"
      },
      "source": [
        "and this is the value of pixel (20,30) in the output image.  \n",
        "\n",
        "We need do this for every pixel in the image, and we could use a pair of nested `for` loops but that's not very efficient.  We can use optimised code in OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neuroo-3zu5U"
      },
      "outputs": [],
      "source": [
        "smoothed = mona.convolve(kernel)\n",
        "smoothed.disp();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvh-DjI0zu5U"
      },
      "source": [
        "**Q: Vary the dimensions of the kernel to see what effect it has?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR9ep_HWzu5V"
      },
      "source": [
        "## Gaussian blur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YPoW8kBzu5V"
      },
      "source": [
        "For image smoothing it is preferable to use a kernel that is isotropic and symmetric such as a 2D Gaussian\n",
        "\n",
        "$G(u,v) = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{u^2+v^2}{2\\sigma^2}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8DKUddwzu5V"
      },
      "outputs": [],
      "source": [
        "w = 5\n",
        "k = range(-w, w+1)\n",
        "sigma = 2\n",
        "[U,V] = np.meshgrid(k, k)\n",
        "kernel = 1/(2*math.pi*sigma**2)*np.exp(-(U**2+V**2)/(2*sigma**2))\n",
        "kernel\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from matplotlib import cm\n",
        "plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "surf = ax.plot_surface(U, V, kernel, cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "# Add a color bar which maps values to colors.\n",
        "plt.colorbar(surf, ax=ax)\n",
        "ax.set_xlabel('U')\n",
        "ax.set_ylabel('V')\n",
        "ax.set_zlabel('K(U,V)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhpFHbo_zu5W"
      },
      "source": [
        "We can blur our image with this kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3bI52ktzu5W"
      },
      "outputs": [],
      "source": [
        "smoothed = mona.convolve(kernel)\n",
        "smoothed.disp();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyOlc1H9zu5W"
      },
      "source": [
        "We can do this in a single step where we pass in the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioSUtvoozu5W"
      },
      "outputs": [],
      "source": [
        "blur = mona.smooth(2, 5)\n",
        "blur.disp();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UsnVaaezu5X"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "* **Vary the dimensions of the kernel to see what effect it has**\n",
        "* **Vary the standard deviation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvASpbhzu5X"
      },
      "source": [
        "## Finding edges\n",
        "We can use 2D filtering to find edges as well.  This convolution kernel will find vertical edges.  The intuition is that each row of this kernel subtracts the pixel to the left from the pixel to the right, which will give a positive value if the intensity is increasing left to right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dov2HDEHzu5X"
      },
      "outputs": [],
      "source": [
        "kernel = np.array( [ [1, 0, -1],\n",
        "                     [2, 0, -2],\n",
        "                     [1, 0, -1] ]) / 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgqJSBruzu5X"
      },
      "source": [
        "<p style=\"border:3px; border-style:solid; border-color:#FF0000; padding: 1em;\">You may often see this filter kernel written with the first and third columns swapped.  This is appropriate if you perform correlation, not convolution. These are two similar operations but differ in the kernel being reflected about its centre.  Many kernels are symmetric which means that convolution and correlation are the same.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8peumPczu5Y"
      },
      "outputs": [],
      "source": [
        "penguins = Image.Read('penguins.png', grey=True, dtype='float')\n",
        "penguins.disp();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNmsxH68zu5Y"
      },
      "outputs": [],
      "source": [
        "gradient_horizontal = penguins.convolve(kernel)\n",
        "gradient_horizontal.disp(colormap='signed');                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8zczdhzu5Y"
      },
      "source": [
        "The image is displayed with a color map that shows negative numbers as red and positive numbers as blue.  Zoom in on the outline of the \"P\" (use the second button from the right in the bottom toolbar) and you can see that the intensity goes up (blue) on the left side of the \"P\", from the grey background to the white paint. It goes down (red) on right of the stem, from the white paint to the gray background.\n",
        "\n",
        "<p style=\"border:3px; border-style:solid; border-color:#FF0000; padding: 1em;\">Our original image comprised unsigned integers (uint8) which are unable to express negative values.  The output of convolve() is a signed floating point image meaning the result at each pixel, can be positive or negative.</p>\n",
        "\n",
        "We can find the horizontal edges by finding vertical gradient, using the transpose of the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkQzr7-zu5Z"
      },
      "outputs": [],
      "source": [
        "gradient_vertical = penguins.convolve(kernel.T)\n",
        "gradient_vertical.disp(colormap='signed');    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86KrKDQYzu5a"
      },
      "source": [
        "# Harris corner features\n",
        "## From first principles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSN7ZQULzu5a"
      },
      "source": [
        "We now have all the ingredients (and knowledge) needed to find interest points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoHQMxJNzu5b"
      },
      "outputs": [],
      "source": [
        "castle = Image.Read('castle.png', grey=True, dtype='float')\n",
        "castle.disp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeWq8RK0zu5b"
      },
      "source": [
        "The fundamental intuition behind interest points is that they have a high gradient in two orthogonal directions, but not necessarily the u- and v-axis directions.\n",
        "\n",
        "However we start by computing the gradient in the u- and v-axis directions and then for every pixel compute this matrix\n",
        "\n",
        "$\\mathbf{A} = \\begin{pmatrix} \\mathbf{G}_\\sigma * \\mathbf{I}_u^2  & \\mathbf{G}_\\sigma * \\mathbf{I}_u \\mathbf{I}_v \\\\ \\mathbf{G}_\\sigma * \\mathbf{I}_u \\mathbf{I}_v & \\mathbf{G}_\\sigma * \\mathbf{I}_v^2 \\end{pmatrix}$\n",
        "\n",
        "This $2 \\times 2$ symmetric matrix is called the structure tensor.\n",
        "It captures the intensity structure of the local neighborhood and its eigenvalues provide a rotationally invariant description of the neighborhood. The elements of the $\\mathbf{A}$ matrix are computed from the image gradients, squared or multiplied, and then smoothed using a weighting matrix. The latter reduces noise and improves the stability and reliability of the detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KN3aO2WBzu5b"
      },
      "outputs": [],
      "source": [
        "# compute derivatives\n",
        "\n",
        "kernel = Kernel.Sobel()\n",
        "\n",
        "Iu = castle.convolve(kernel)\n",
        "Iv = castle.convolve(kernel.T)\n",
        "\n",
        "# make a Gaussian smoothing kernel 11x11 with sigma=1\n",
        "w2 = 5  # half width\n",
        "k = range(-w2, w2+1)\n",
        "sigma = 1\n",
        "[U,V] = np.meshgrid(k, k)\n",
        "kgaussian = 1.0 / (2 * math.pi * sigma**2) * np.exp(-(U**2 + V**2) / (2 * sigma**2))\n",
        "\n",
        "# could also use kgaussian = kgauss(sigma, w2)\n",
        "\n",
        "# compute the 3 unique elements of the structure tensor\n",
        "Ix = (Iu * Iu).convolve(kgaussian)\n",
        "Iy = (Iv * Iv).convolve(kgaussian)\n",
        "Ixy = (Iu * Iv).convolve(kgaussian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qS6voZgzu5c"
      },
      "source": [
        "An interest point $(u, v)$ is one where whatever direction we move the window it rapidly becomes dissimilar to the original region. If we consider the original image $\\mathbf{I}$ as a surface the eigenvalues of $\\mathbf{A}$ are the principal curvatures of the surface at that point: \n",
        "\n",
        "* If both eigenvalues are small then the surface is flat, that is the image region has approximately constant local intensity. \n",
        "* If one eigenvalue is high and the other low, then the surface is ridge shaped which indicates an edge. \n",
        "* If both eigenvalues are high the surface is sharply peaked which we consider to be a corner.\n",
        "\n",
        "The well known Shi-Tomasi detector considers the strength of the corner, or cornerness, as the minimum eigenvalue\n",
        "\n",
        "$C_{\\text{ST}}(u,v) = \\min( \\lambda_1, \\lambda_2)$\n",
        "\n",
        "where $\\lambda_i$ are the eigenvalues of $\\mathbf{A}$. Points in the image for which this measure is high are referred to as [\"good features to track\"](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf). \n",
        "\n",
        "The Harris detector is based on this same insight but defines corner strength as\n",
        "\n",
        "$C_{H}(u,v) = \\det(\\mathbf{A}) - k \\text{trace}(\\mathbf{A})$\n",
        "\n",
        "and again a large value represents a strong, distinct, corner. Since $\\det(A) = \\lambda_1  \\lambda_2$ and\n",
        "$\\text{trace}(A) = \\lambda_1 + \\lambda_2$ the Harris detector responds when both eigenvalues are large and elegantly avoids computing the eigenvalues of A which has a somewhat higher computational cost.  Typically $k=0.04$.\n",
        "\n",
        "We compute this for all pixels at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBUHD26dzu5c"
      },
      "outputs": [],
      "source": [
        "rawC = (Ix * Iy - Ixy * Ixy) - 0.04 * (Ix + Iy)**2\n",
        "rawC.disp(colormap='signed');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLiSZ45Uzu5c"
      },
      "source": [
        "which shows the raw value of $C_H$. If we zoom in and examine the image we will see that large areas of smooth background are around zero, edges are negative and corners of things (like the letters) have a positive value.\n",
        "\n",
        "Next we need to find the coordinates of those positive patches values.  For every pixel we find the largest value in a $5\\times 5$ window around each pixel but not including the centre pixel itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "URXGVC_vzu5d"
      },
      "outputs": [],
      "source": [
        "# create the structuring element\n",
        "SE = np.ones((5,5), np.uint8)\n",
        "SE[2,2] = 0\n",
        "\n",
        "# find the maximum value around each pixel\n",
        "surrounding_max = rawC.dilate(SE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ8eT2HMzu5d"
      },
      "source": [
        "We've done this using a morphological filtering operation called dilation, you can find more details from the documentation page for [`dilate`](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c).\n",
        "\n",
        "Next we find all the pixels whose value is greater than those surrounding it (which we just computed). This is a common trick in computer vision when we are looking for peaks &ndash; it's called non-local maxima suppresion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aGCCwBPzu5d"
      },
      "outputs": [],
      "source": [
        "uv = (rawC > surrounding_max).nonzero()  # find the indices of all the peaks\n",
        "peakvals = rawC.image[uv[1,:], uv[0,:]]  # find the peak values\n",
        "k = np.argsort(-peakvals)  # sort those values into descending order\n",
        "print('%d peaks found\\n' % (len(k),))  # report how many peaks were found\n",
        "k = k[:200]  # take first 200, these are the strongest peaks\n",
        "\n",
        "# find the u, v coordinates as two separate lists\n",
        "u = np.array([uv[0][i] for i in k])\n",
        "v = np.array([uv[1][i] for i in k])\n",
        "\n",
        "# display the image, and overlay with markers for each detected corner feature\n",
        "castle.disp(darken=True, block=None);\n",
        "plot_point((u, v), 'y+');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BBbrRIvzu5e"
      },
      "source": [
        "Zoom in on the letters and you will see that the features are placed on sharp corners.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Developed by Chris Harris and Mike Stephens in 1988 the [Harris detector](https://en.wikipedia.org/wiki/Harris_Corner_Detector) was THE detector used in computer vision until\n",
        "[SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform) was developed by David Lowe in 1999.\n",
        "SIFT is very powerful (it has the very useful property called scale-invariance which we won't have time to cover) but was [patented by the University of British Columbia](https://patents.google.com/patent/US6711293) and researchers were therefore reluctant to be too dependent on it. [SURF](https://en.wikipedia.org/wiki/Speeded_up_robust_features) was  developed by Herbert Bay, Tinne Tuytelaars, and Luc Van Gool and published in 2006. It has similar functionality to SIFT but is much faster.  It turns out that [SURF was also patented](https://worldwide.espacenet.com/patent/search/family/036954920/publication/US2009238460A1?q=pn%3DUS2009238460) but people seemed to worry less about that.  Now the SIFT patent has expired so it is back in favour.  OpenCV ships with SIFT but not SURF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKETKTF_zu5e"
      },
      "source": [
        "# SIFT features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RBNZgm0zu5e"
      },
      "outputs": [],
      "source": [
        "butterfly = Image.Read('butterfly.jpg')\n",
        "butterfly.disp();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVCIPAtmzu5e"
      },
      "source": [
        "Now we can find SIFT features in this scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xrTcjTrszu5e"
      },
      "outputs": [],
      "source": [
        "features = butterfly.SIFT()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp6HbEZBzu5f"
      },
      "source": [
        "which is another type of object that contains data about all the features, it behaves a bit like a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw6ha2Oszu5f"
      },
      "outputs": [],
      "source": [
        "len(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuWcaoCpzu5f"
      },
      "source": [
        "and in this case there are 1055 features found.\n",
        "\n",
        "Each feature (sometimes called keypoints) has a number of characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qgnd_spzu5f"
      },
      "outputs": [],
      "source": [
        "features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf1hXxNezu5f"
      },
      "source": [
        "which are also attributes of the object.  For example the centroid is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhMxtoA5zu5f"
      },
      "outputs": [],
      "source": [
        "features[0].p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLjAzUCzu5f"
      },
      "source": [
        "Unlike the Harris feature, the SIFT feature also has a size (in pixels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu_rwnXxzu5g"
      },
      "outputs": [],
      "source": [
        "features[0].scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZnHEzBfzu5g"
      },
      "source": [
        "as well as a characteristic angle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrBa7fgozu5g"
      },
      "outputs": [],
      "source": [
        "features[0].orientation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MyGyz9azu5g"
      },
      "source": [
        "in degrees.\n",
        "\n",
        "Each feature also strength or response (similar to \"cornerness\" for the Harris detector) that indicates how unique the feature is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2fGdtV2zu5g"
      },
      "outputs": [],
      "source": [
        "features[0].strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWmYXsAszu5g"
      },
      "source": [
        "The first 10 SIFT features are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m8xzaqYzu5g"
      },
      "outputs": [],
      "source": [
        "features[:10].table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsiOyCV-zu5g"
      },
      "source": [
        "and they are essentially in the order they were encountered in the image.  We are often interested in the N strongest features, so we would first sort the features into descending order by strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RNTWOTL6zu5g"
      },
      "outputs": [],
      "source": [
        "features = features.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSQMy7mlzu5h"
      },
      "source": [
        "and now the first 10 are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQR_AlsLzu5h"
      },
      "outputs": [],
      "source": [
        "features[:10].table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2yFAlKCzu5h"
      },
      "source": [
        "We can plot the features, their size and orientation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A151FQJ0zu5h"
      },
      "outputs": [],
      "source": [
        "butterfly.disp(block=None)\n",
        "features.sort(by='scale')[:50].plot(filled=True, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T56heJfFzu5h"
      },
      "source": [
        "where each feature is denoted by a translucent disk that indicates the position and scale of the feature.  Clearly it has found the distinctive points in the image.\n",
        "\n",
        "**Q: Compute and plot a histogram of the feature scales.**\n",
        "\n",
        "Each feature point has a descriptor which is a summary of the pixel values within the variable size disk surrounding it. This is given by the descriptor which is a vector of 64 floats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfSWo6QUzu5h"
      },
      "outputs": [],
      "source": [
        "features[0].descriptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6egtJLuzu5i"
      },
      "source": [
        "If we moved the camera to shrink the image by a factor of two the size of all the support region disks would reduce by a factor of two, but the descriptor would remain approximately constant.  If we rotated the camera by 90degrees the characteristic angle of all the features would change by 90degrees but the feature size and the descriptor would remain approximately constant.\n",
        "\n",
        "We can display the orientation, along with the feature size and scale by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9LZvqgzu5i"
      },
      "outputs": [],
      "source": [
        "butterfly.disp(block=None)\n",
        "features.sort(by='scale')[:50].plot(filled=True, alpha=0.5, hand=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx-IVNWyzu5i"
      },
      "source": [
        "where the blue line (like a one-handed clock) indicates the characteristic orientation.  Some features are doubled up, same position and scale, but different orientation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWtuQDZwzu5i"
      },
      "source": [
        "# Finding correspondences between images\n",
        "\n",
        "First we will load two different views of the same scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-cdFeY4zu5i"
      },
      "outputs": [],
      "source": [
        "im1 = Image.Read('eiffel-1.png')\n",
        "im2 = Image.Read('eiffel-2.png')\n",
        "Image.Hstack((im1,im2)).disp()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR-varofRBWp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEo8EqT3Qv-x"
      },
      "source": [
        "and then use SIFT to find corresponding features in the two images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "md1S7iLHQweC"
      },
      "outputs": [],
      "source": [
        "f1 = im1.SIFT()\n",
        "f2 = im2.SIFT()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT7AIsA7zu5i"
      },
      "source": [
        "Then we create a \"*match*\" object and give it the two sets of features.  It finds the best matches and for each match computes a `distance` which is a measure of dissimilarity.  We sort the matches into decreasing similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFT_GbQKzu5j"
      },
      "outputs": [],
      "source": [
        "matches = f1.match(f2)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZQtJ7mPzu5j"
      },
      "source": [
        "then plot the best 100 matches.  The `plot` method renders the matches onto the original images and overlays lines that connect corresponding points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM5XWcKzzu5j"
      },
      "outputs": [],
      "source": [
        "matches[:100].plot('y', alpha=0.6, block=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDIAtRqSzu5j"
      },
      "source": [
        "Zoom in and see how well it has done.\n",
        "\n",
        "Finding corresponding points in a pair of images is critical to a lot of important computer vision problems such as stereo, structure from motion and visual SLAM."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "image-features.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "181.59375px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
